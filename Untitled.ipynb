{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "MNIST dumped into mnist_data.pkl\n",
      "Data loaded.\n",
      "Shapes of data are as follows: \n",
      "Train data shape: (50400, 784)\n",
      "Train labels shape: (10, 50400)\n",
      "Test data shape: (10000, 784)\n",
      "Test labels shape: (10, 10000)\n",
      "Validation data shape: (9600, 784)\n",
      "Validation labels shape: (10, 9600)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-68dd30c14320>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[0mimlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[0mlblist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m \u001b[0mdirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[0mdirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import imageio\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "from scipy.misc import imsave\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#Let's load the MNIST dataset and store it as numpy arrays in a pickle file for later use.\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "\n",
    "def maybe_download(filename):\n",
    "    \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "    if not tf.gfile.Exists(WORK_DIRECTORY):\n",
    "        tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
    "    filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "    if not tf.gfile.Exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    with tf.gfile.GFile(filepath) as f:\n",
    "        size = f.size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        #data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "def save_mnist_data():\n",
    "    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # Extract it into np arrays.\n",
    "    train_data = extract_data(train_data_filename, 60000)\n",
    "    train_labels = extract_labels(train_labels_filename, 60000)\n",
    "    test_data = extract_data(test_data_filename, 10000)\n",
    "    test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "    #Pickle these variables\n",
    "    mnist_data = {\n",
    "        \"train_data\": train_data,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"test_data\": test_data,\n",
    "        \"test_labels\": test_labels}\n",
    "\n",
    "    pickle.dump(obj = mnist_data, file=open('mnist_data.pkl','wb'),protocol=3)\n",
    "    print(\"MNIST dumped into mnist_data.pkl\")\n",
    "save_mnist_data()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#Function to transform data into the format required\n",
    "#Takes data of the form [60000 x 28 x 28 x 1] and returns [60000 x 784]\n",
    "def transform_data(data):\n",
    "    a = np.reshape(data,newshape=(data.shape[0],data.shape[1]*data.shape[1]))\n",
    "    return a\n",
    "def load_mnist_data(filename = 'mnist_data.pkl',standardize = True):\n",
    "    mnist_data = pickle.load(open('mnist_data.pkl','rb'))\n",
    "    train_data = transform_data(mnist_data[\"train_data\"])\n",
    "    train_labels = mnist_data[\"train_labels\"].reshape(mnist_data[\"train_labels\"].shape[0],-1)\n",
    "    test_data = transform_data(mnist_data[\"test_data\"])\n",
    "    test_labels = mnist_data[\"test_labels\"].reshape(mnist_data[\"test_labels\"].shape[0],-1)\n",
    "    #Form validation set\n",
    "    train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, \n",
    "                                                                          train_labels, \n",
    "                                                                          test_size=0.16, \n",
    "                                                                          random_state=42)\n",
    "    #Standardize the data - divide by 255\n",
    "    if standardize:\n",
    "        train_data, valid_data, test_data = train_data/255., valid_data/255., test_data/255.\n",
    "    train_data, valid_data, test_data = train_data, valid_data, test_data\n",
    "    \n",
    "    return train_data,train_labels,valid_data,valid_labels, test_data, test_labels\n",
    "train_data,train_labels,valid_data, valid_labels, test_data,test_labels = load_mnist_data()\n",
    "\n",
    "#One hot encode the labels\n",
    "train_labels = np.array(pd.get_dummies(train_labels.flatten())).T\n",
    "valid_labels = np.array(pd.get_dummies(valid_labels.flatten())).T\n",
    "test_labels = np.array(pd.get_dummies(test_labels.flatten())).T\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(\"Shapes of data are as follows: \")\n",
    "print(\"Train data shape: \"+ str(train_data.shape))\n",
    "print(\"Train labels shape: \"+ str(train_labels.shape))\n",
    "print(\"Test data shape: \"+ str(test_data.shape))\n",
    "print(\"Test labels shape: \"+ str(test_labels.shape))\n",
    "print(\"Validation data shape: \"+ str(valid_data.shape))\n",
    "print(\"Validation labels shape: \"+ str(valid_labels.shape))\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "#Load USPS Data\n",
    "import numpy as np\n",
    "import random as ran\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "\n",
    "'''\n",
    "In this code, I try to retrieve USPS data from a directory structure\n",
    "where name of the directort is the label for the files in that directory\n",
    "'''\n",
    "\n",
    "path = './images/Numerals/'\n",
    "\n",
    "# Initialize 2 lists for labels and images\n",
    "imlist = []\n",
    "lblist = []\n",
    "dirs = list(os.walk(path))[0]\n",
    "dirs = dirs[1]\n",
    "for d in dirs:\n",
    "    #print(d)\n",
    "    for files in d:\n",
    "        directory = os.path.join(path,files)\n",
    "        #print(directory)\n",
    "        for fname in os.listdir(directory):\n",
    "            if(fname.endswith(\".png\")):    \n",
    "                image = cv2.imread(os.path.join(path,d,fname))\n",
    "                im_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                im_resize = cv2.resize(im_gray, (28, 28))\n",
    "                #Letters are colored in the opposite way when compared to MNIST\n",
    "                imlist.append(255 - np.ravel(im_resize)) \n",
    "                lblist.append(int(d))\n",
    "\n",
    "# Normalize the values in the image\n",
    "images = np.array(imlist)/255.\n",
    "labels = np.reshape(np.array(lblist),[-1,1]).flatten()\n",
    "# Get one-hot representation\n",
    "labels = np.array(pd.get_dummies(labels))\n",
    "\n",
    "# Set a random seed to get reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomize the input data\n",
    "ids = list(range(0,len(labels)))\n",
    "np.random.shuffle(ids)\n",
    "usps_images_shuf = images[ids]\n",
    "usps_labels_shuf = labels[ids]\n",
    "print(\"USPS Data loaded.\")\n",
    "print(\"USPS dataset shape: \", str(usps_images_shuf.shape))\n",
    "print(\"USPS labels shape: \", str(usps_labels_shuf.shape))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "#set up params\n",
    "sess = tf.Session()\n",
    "seed = 2\n",
    "tf.set_random_seed(2)\n",
    "batch_size = 100\n",
    "learning_rate = 1e-4\n",
    "m = 784\n",
    "epochs = 1000\n",
    "n = train_data.shape[1]\n",
    "k = 10 #Number of classes\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, m])\n",
    "y_orig = tf.placeholder(tf.float32, [None, k])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([m, k]))\n",
    "b = tf.Variable(tf.zeros([k]))\n",
    "print(\"W.shape: \" + str(W.shape))\n",
    "print(\"b.shape: \" + str(b.shape))\n",
    "#let Y = Wx + b with a softmax activiation function\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_orig * tf.log(y), reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "losses = []\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "test_accs = []\n",
    "usps_accs = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "            current_x, current_y = mnist.train.next_batch(batch_size)\n",
    "            _,c = sess.run([optimizer,cross_entropy], feed_dict={x: current_x, y_orig: current_y})\n",
    "            epoch_loss += c\n",
    "        print(\"Loss at epoch %d: %.3f\" %(epoch,epoch_loss))\n",
    "        losses.append(epoch_loss)\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_orig,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        train_accuracy_mnist = accuracy.eval({x:mnist.train.images, y_orig: mnist.train.labels})\n",
    "        test_accuracy_mnist = accuracy.eval({x:mnist.test.images, y_orig: mnist.test.labels})\n",
    "        valid_accuracy_mnist = accuracy.eval({x:mnist.validation.images, y_orig: mnist.validation.labels})\n",
    "        usps_accuracy = accuracy.eval({x:usps_images_shuf, y_orig: usps_labels_shuf})\n",
    "        print('Train accuracy:',train_accuracy_mnist)\n",
    "        print('Validation accuracy:',valid_accuracy_mnist)\n",
    "        print('Test accuracy:',test_accuracy_mnist)\n",
    "        print('USPS Accuracy:',usps_accuracy)\n",
    "        train_accs.append(train_accuracy_mnist)\n",
    "        test_accs.append(test_accuracy_mnist)\n",
    "        valid_accs.append(valid_accuracy_mnist)\n",
    "        usps_accs.append(usps_accuracy_mnist)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x_plot = list(range(epochs))\n",
    "plt.plot(x_plot, train_accs,'r',x_plot, valid_accs, 'b', x_plot, test_accs, 'g',x_plot,usps_accs,'y')\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_path,y_path):\n",
    "    Ms = [5,6,7,8]\n",
    "    learning_rates = [0.05,0.08,0.1]\n",
    "    L2_lambdas = [0.5,0.6]\n",
    "\n",
    "    best_valid_error = float(\"inf\")\n",
    "    grid_search_dict = {}\n",
    "    count = 0\n",
    "    for i, M_val in enumerate(Ms):\n",
    "        for j,L2_lambda_val in enumerate(L2_lambdas):\n",
    "            for k,learning_rate_val in enumerate(learning_rates):\n",
    "                print(\"Running SGD...\")\n",
    "                print(\"L2_Lambda: \" + str(L2_lambda_val))\n",
    "                print(\"learning rate: \" + str(learning_rate_val))\n",
    "                print(\"M: \" + str(M_val+1))\n",
    "                w_cf, w_sgd, validation_error, test_error, errors = LinearRegression(X_path, \n",
    "                             y_path,\n",
    "                             M=M_val, \n",
    "                             learning_rate = learning_rate_val,\n",
    "                             L2_lambda = L2_lambda_val, \n",
    "                             num_epochs = 5000, \n",
    "                             minibatch_size = 512\n",
    "                            )\n",
    "                grid_search_dict[count] = {}\n",
    "                grid_search_dict[count]['M'] = M_val\n",
    "                grid_search_dict[count]['L2_Lambda'] = L2_lambda_val\n",
    "                grid_search_dict[count]['learning_rate'] = learning_rate_val\n",
    "                grid_search_dict[count]['errors'] = errors\n",
    "                grid_search_dict[count]['validation_error'] = validation_error\n",
    "                grid_search_dict[count]['test_error'] = test_error\n",
    "                grid_search_dict[count]['weights'] = w_sgd\n",
    "            \n",
    "                if validation_error < best_valid_error:\n",
    "                    best_valid_error = validation_error\n",
    "                    best_test_error = test_error\n",
    "                    best_weights_sgd = w_sgd\n",
    "                    best_learning_rate = learning_rate_val\n",
    "                    best_M = M_val\n",
    "                    best_L2_lambda = L2_lambda_val\n",
    "                count += 1\n",
    "    return best_M, best_L2_lambda, best_learning_rate,best_valid_error, best_test_error, best_weights_sgd, grid_search_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
